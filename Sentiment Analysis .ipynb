{"cells": [{"metadata": {}, "cell_type": "code", "source": "pip install nltk==3.3", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: nltk==3.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (3.3)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from nltk==3.3) (1.15.0)\nNote: you may need to restart the kernel to use updated packages.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import nltk\nnltk.download('twitter_samples')", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /home/wsuser/nltk_data...\n[nltk_data]   Unzipping corpora/twitter_samples.zip.\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from nltk.corpus import twitter_samples\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\ntext = twitter_samples.strings('tweets.20150430-223406.json')", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "nltk.download('punkt')", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package punkt to /home/wsuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 21, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from nltk.corpus import twitter_samples\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\ntext = twitter_samples.strings('tweets.20150430-223406.json')\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json') [0]\n#print(tweet_tokens[0])", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "#FollowFriday\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#Normalization \nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package wordnet to /home/wsuser/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/wsuser/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from nltk.tag import pos_tag\nfrom nltk.corpus import twitter_samples\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(pos_tag(tweet_tokens[0]))\n", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "...\n\nfrom nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\ndef lemmatize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\n\n#print(lemmatize_sentence(tweet_tokens[0]))\n\n...\n\nimport re, string\n\ndef remove_noise(tweet_tokens, stop_words = ()):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(tweet_tokens):\n        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens", "execution_count": 27, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "nltk.download('stopwords')", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package stopwords to /home/wsuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "...\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n#print(remove_noise(tweet_tokens[0], stop_words))", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nnegative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\npositive_cleaned_tokens_list = []\nnegative_cleaned_tokens_list = []\n\nfor tokens in positive_tweet_tokens:\n    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n\nfor tokens in negative_tweet_tokens:\n    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n#print(positive_tweet_tokens[500])\n#print(positive_cleaned_tokens_list[500])", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n['dang', 'rad', '#fanart', ':d']\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "def get_all_words(cleaned_tokens_list):\n    for tokens in cleaned_tokens_list:\n        for token in tokens:\n            yield token\n\nall_pos_words = get_all_words(positive_cleaned_tokens_list)", "execution_count": 33, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from nltk import FreqDist\n\nfreq_dist_pos = FreqDist(all_pos_words)\nprint(freq_dist_pos.most_common(10))", "execution_count": 34, "outputs": [{"output_type": "stream", "text": "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#BuildingDataSet\ndef get_tweets_for_model(cleaned_tokens_list):\n    for tweet_tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tweet_tokens)\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)", "execution_count": 35, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "...\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]", "execution_count": 36, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "from nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))", "execution_count": 37, "outputs": [{"output_type": "stream", "text": "Accuracy is: 0.9943333333333333\nMost Informative Features\n                      :( = True           Negati : Positi =   2048.9 : 1.0\n                follower = True           Positi : Negati =     34.1 : 1.0\n                  arrive = True           Positi : Negati =     31.4 : 1.0\n                     bam = True           Positi : Negati =     23.3 : 1.0\n                    glad = True           Positi : Negati =     22.6 : 1.0\n                    sick = True           Negati : Positi =     20.7 : 1.0\n                 welcome = True           Positi : Negati =     18.8 : 1.0\n                     sad = True           Negati : Positi =     18.3 : 1.0\n                followed = True           Negati : Positi =     14.8 : 1.0\n                     x15 = True           Negati : Positi =     14.2 : 1.0\nNone\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from nltk.tokenize import word_tokenize\n\ncustom_tweet = \"I am exctied for the new game to come out\"\n\ncustom_tokens = remove_noise(word_tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))", "execution_count": 41, "outputs": [{"output_type": "stream", "text": "Positive\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "custom_tweet2 = \"I hate this\"\n\ncustom_tokens2 = remove_noise(word_tokenize(custom_tweet2))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens2)))", "execution_count": 43, "outputs": [{"output_type": "stream", "text": "Negative\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "custom_tweet3 = \"Excited for the new Marvel Movie to come out\"\n\ncustom_tokens3 = remove_noise(word_tokenize(custom_tweet3))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens3)))", "execution_count": 44, "outputs": [{"output_type": "stream", "text": "Positive\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "custom_tweet4 = \"Lame\"\n\ncustom_tokens4 = remove_noise(word_tokenize(custom_tweet4))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens4)))", "execution_count": 47, "outputs": [{"output_type": "stream", "text": "Negative\n", "name": "stdout"}]}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}